{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression: Ordinary least squares\n",
    "\n",
    "In this notebook we will go over the concept of ordinary least squares estimation for linear regression.  For a deeper dive into regression analysis, I would recommend [\"The Truth about Linear Regression\" by Cosma Shalizi](http://www.stat.cmu.edu/~cshalizi/TALR/).\n",
    "\n",
    "In linear regression, we want to find the line:\n",
    "\n",
    "$$\n",
    "y = m*x + b\n",
    "$$\n",
    "\n",
    "that best relates two variables $x$ (the independent variable, or *predictor*) and $y$ (the dependent variable, or *outcome*).  We define \"best\" as the line that miminizes the squared error loss:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = \\sum_{i=1}^N{(y_i - \\hat{y})^2}\n",
    "$$\n",
    "\n",
    "where $\\hat{y_i} = mx_i + b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from sklearn.metrics import r2_score\n",
    "from balanced_kfold import BalancedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at a simple example, with two\n",
    "simulated variables that are linearly related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "npts = 20\n",
    "noise_sd = 1\n",
    "m = 0.5\n",
    "b = 3\n",
    "X = rng.randn(npts, 1)\n",
    "y = X.dot(m) + b + rng.randn(npts, 1)*noise_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X[:, 0], y[:, 0])[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the regression solution using ordinary least squares\n",
    "\n",
    "One way to understand the estimation of the best fitting line is through the relationship between the regression slope and the correlation coefficient.\n",
    "\n",
    "Remember the formula for the Pearson correlation:\n",
    "\n",
    "$$\n",
    "r = \\frac{covariance_{x,y}}{s_x * s_y} = \\frac{\\sum_{i=1}^N{(x_i - \\bar{x})(y_i - \\bar{y})}}{(N-1)*s_x*s_y}\n",
    "$$\n",
    "\n",
    "where $s_x$ and $s_y$ are the standard deviations for the x and y variables. The estimate of the regression slope for a single independent variable is very similar:\n",
    "\n",
    "$$\n",
    "\\hat{m} = \\frac{covariance_{x,y}}{s^2_x}\n",
    "$$\n",
    "\n",
    "The only difference is that the correlation scales by the product of $s_x$ and $s_y$, whereas the regression slope scales only by the variance of the $x$ variable (i.e. $s^2_x$).  The relation between them is:\n",
    "\n",
    "$$\n",
    "\\hat{m} = r * \\frac{s_y}{s_x}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot that line against our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "\n",
    "\n",
    "def abline(slope, intercept):\n",
    "    \"\"\"\n",
    "    Plot a line from slope and intercept\n",
    "    from: https://stackoverflow.com/questions/7941226/how-to-add-line-based-on-slope-and-intercept-in-matplotlib\n",
    "    \n",
    "    slope: float, slope of line\n",
    "    intercept: float, intercept of line\n",
    "    \"\"\"\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, 'r--')\n",
    "\n",
    "\n",
    "# use ddof=1 argument for std, since we want the sample std\n",
    "# rather than the population std (which is the default for np.std)\n",
    "std_x = np.std(X, ddof=1)\n",
    "std_y = np.std(y, ddof=1)\n",
    "m_hat = np.corrcoef(X.T, y.T)[0, 1] * (std_y/std_x)\n",
    "\n",
    "# compute the intercept plugging in the mean values of X and Y\n",
    "intercept_hat = np.mean(y) - m_hat*np.mean(X)\n",
    "\n",
    "abline(m_hat, intercept_hat)\n",
    "m_hat, intercept_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving OLS using linear algebra\n",
    "\n",
    "\n",
    "In general, we would use linear algebra to estimate the OLS solution, because it is more flexible. Framed as a linear algebra problem:\n",
    "\n",
    "$$\n",
    "\\textbf{Y} = \\textbf{X} \\cdot \\textbf{b} + \\epsilon\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\textbf{X}$ is the $N_{samples} x N_{features}$ *design matrix* containing the predictors (known)\n",
    "- $\\textbf{Y}$ is the $N_{samples} x N_{targets}$ matrix of outcomes to be predicted (known)\n",
    "- $\\textbf{b}$ is the $N_{features} x N_{targets}$ matrix of *regression parameters* (unknown)\n",
    "- $\\epsilon$ is the  $N_{samples} x N_{targets}$ matrix of errors (unknown)\n",
    "\n",
    "In the case of simple linear regression (with a single x and single y variable), $N_{features} = 1$, $N_{targets} = 1$, and $N_{samples}$ is the number of data points.  \n",
    "\n",
    "Using this linear algebra approach is is possible to have multiple Y variables (each of which gets its own set of parameter estimates), but we will primarily work with analyses where there is only a single target variable, as in the case of simple regression.\n",
    "\n",
    "The outcomes $\\textbf{Y}$ and predictors $\\textbf{X}$ are known, and we need to estimate the values of $\\textbf{b}$ that result in the minimum squared error loss.\n",
    "\n",
    "Conceptually, one might imagine estimating $\\textbf{b}$ by solving for it algebraically:\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{b}} = \\frac{\\textbf{Y}}{\\textbf{X}}\n",
    "$$\n",
    "\n",
    "However, since X is a matrix, we can't simply divide by it. However, another way to think about this is that we can multiply $\\textbf{Y}$ by the matrix inverse of $\\textbf{X}$:\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{b}} = \\textbf{X}^{-1} \\cdot \\textbf{Y}\n",
    "$$\n",
    "\n",
    "The problem here is that so far we only know how to compute the inverse of a square matrix, but $\\textbf{X}$ is never going to be square (for reasons we will see later); it must always have more rows than columns.  In order to come up with $\\textbf{X}^{-1}$, we need to use what is called the Moore-Penrose pseudoinverse, denoted $\\textbf{X}^{+}$:\n",
    "\n",
    "$$\n",
    "\\textbf{X}^{+} = (X^T \\cdot X)^{-1} \\cdot X^T\n",
    "$$\n",
    "\n",
    "for the particular case where $\\textbf{X}$ has linearly independent columns and more rows than columns.  This produces a *left pseudoinverse*, meaning that if we want to multiply another vector by it, the pseudoinverse must be on the left.  We can now use this to generate the solution to our parameter estimation problem:\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{b}} = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot \\textbf{Y}\n",
    "$$\n",
    "\n",
    "Because $(X^T \\cdot X)^{-1}$ is the covariance matrix of X, and $ X^T \\cdot \\textbf{Y}$ is the covariance between $\\textbf{X}$ and $\\textbf{y}$, we can view this as a multivariate generalization of the estimation formula above:\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{b}} = \\frac{cov(\\textbf{X}, \\textbf{Y})}{var(\\textbf{X})}\n",
    "$$\n",
    "\n",
    "To estimate this, we need to allow the model to also estimate an intercept term.  This is basically a constant offset for all data points, so we can model this by adding a columns of ones to X. It is common in some fields to display the design matrix as an image, where the intensity relates to the magnitude of the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an intercept term to X\n",
    "X_int = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "\n",
    "# display design matrix as image\n",
    "plt.imshow(X_int, cmap='gray', aspect='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's estimate the parameters using the pseudoinverse.  We can conceptually separate this out into a matrix that *projects* the data into the space of the design matrix; we will call this $H$, since it is often referred to as the *hat matrix* because it puts the \"hat\" onto beta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hat matrix\n",
    "H = np.linalg.inv(X_int.T.dot(X_int)).dot(X_int.T)\n",
    "\n",
    "# estimate the parameters\n",
    "b_hat = H.dot(y)\n",
    "\n",
    "b_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the same result using the sklearn `LinearRegression` object.  This object by default automatically fits an intercept term, so we should turn that off since the intercept is already in our design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr.fit(X_int, y)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view least squares estimation in geometric terms.  If we view our data as a vector in an N-dimensional space (where each data point in the vector defines a different dimension), then we can view the job of linear regression as projecting that N-dimensional space into a new space that has the same number of dimensions as there are predictors in the model (i.e. the number of columns in the design matrix).\n",
    "\n",
    "Let's first create a simple dataset, with three observations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[3, 2, 2]]).T  # data to plot\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot those data in a three-dimensional space, where each data point is treated as a dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca(projection='3d')  # to work in 3d\n",
    "\n",
    "# Make data for grid\n",
    "Xgrid, Ygrid = np.meshgrid(np.arange(-3, 3, 0.25), np.arange(-3, 3, 0.25))\n",
    "Zgrid = np.zeros(Xgrid.shape)\n",
    "\n",
    "# Plot the surface.\n",
    "ax.plot_surface(Xgrid, Ygrid, Zgrid, color='white',\n",
    "                linewidth=0, antialiased=False, alpha=0.2)\n",
    "\n",
    "# set up variables for plotting\n",
    "x = np.array([[1, 0, 0], [0, 1, 0]])  # basis vectors\n",
    "\n",
    "# plot basis vectors\n",
    "for i in range(x.shape[0]):\n",
    "    ax.plot3D([0, x[i, 0]], [0, x[i, 1]], [0, x[i, 2]], 'blue', )\n",
    "\n",
    "# plot data vector\n",
    "ax.plot3D([0, y[0, 0]], [0, y[1, 0]], [0, y[2, 0]], 'green',)\n",
    "ax.scatter3D(y[0, 0], y[1, 0], y[2, 0], cmap='Greens')\n",
    "\n",
    "elev = 10\n",
    "azim = 60\n",
    "ax.view_init(elev, azim)  # set viewpoint\n",
    "plt.xlabel('X0')\n",
    "plt.ylabel('X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind least squares estimation is to project the $N_{samples}$ dimensional data into an $N_{features}$ dimensional space. In this example we will project the data into a two-dimensional space, represented as the plane containing the blue lines in the figure above.  To do this, we need to create a design matrix to represent the projection from the full space of the data into the restricted space of the design.\n",
    "\n",
    "To do this, we create a design matrix X with two columns; the two columns should be linearly independent, and each column should have equal norm (which we will set to one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "# normalize\n",
    "X = X / np.linalg.norm(X[:, 0])\n",
    "plt.imshow(X, cmap='gray', aspect='auto')\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use our formula from above to estimate $\\hat{b}$ from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "print(bhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the estimated parameters in our space (the dotted black line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca(projection='3d')               # to work in 3d\n",
    "\n",
    "# Plot the surface.\n",
    "ax.plot_surface(Xgrid, Ygrid, Zgrid, color='white',\n",
    "                linewidth=0, antialiased=False, alpha=0.2)\n",
    "\n",
    "# set up variables\n",
    "y = np.array([[3, 2, 2]]).T\n",
    "x = np.array([[1, 0, 0], [0, 1, 0]])\n",
    "\n",
    "\n",
    "# plot basis vectors\n",
    "for i in range(x.shape[0]):\n",
    "    ax.plot3D([0, x[i, 0]], [0, x[i, 1]], [0, x[i, 2]], 'blue', )\n",
    "\n",
    "# plot data vector\n",
    "ax.plot3D([0, y[0, 0]], [0, y[1, 0]], [0, y[2, 0]], 'green',)\n",
    "ax.scatter3D(y[0, 0], y[1, 0], y[2, 0], cmap='Greens')\n",
    "\n",
    "# plot projection vector\n",
    "ax.plot3D([0, bhat[0]], [0, bhat[1]], [0, 0], 'k--')\n",
    "\n",
    "# plot error vector\n",
    "ax.plot3D([bhat[0], y[0, 0]], [bhat[1], y[1, 0]], [0, y[2, 0]], 'r--')\n",
    "\n",
    "ax.view_init(elev, azim)  # set viewpoint\n",
    "plt.xlabel('X0')\n",
    "plt.ylabel('X1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the original data vector (green line) has been decomposed into the projection into the column space of the design (black line), and the residual error (red line).  Let's confirm that the combination of the norms of these vectors (by the Pythagorean theorem) is the same as the norm of the original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_y = np.linalg.norm(y)\n",
    "norm_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_yhat = np.linalg.norm(X.dot(bhat))\n",
    "norm_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = y - X.dot(bhat)\n",
    "norm_resid = np.linalg.norm(resid)\n",
    "norm_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(norm_y, np.sqrt(norm_yhat**2 + norm_resid**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geometric view also provides a way to understand how well the model fits the data.  Let's fit the model using the `OLS` function from the statsmodels package, which provides additional fit statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the R-squared for the model is 0.824.  Remember that the R-squared is computed as:\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{SS_{model}}{SS_{total}} \n",
    "$$\n",
    "\n",
    "We can compute these sums of squares by simply squaring the norm of the two relevant vectors:\n",
    "\n",
    "$$\n",
    "SS_{total} = \\|Y\\|^2\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "SS_{model} = \\|\\hat{Y}\\|^2\n",
    "$$\n",
    "\n",
    "and thus:\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\|\\hat{Y}\\|^2}{\\|Y\\|^2}\n",
    "$$\n",
    "\n",
    "which we can confirm here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(norm_yhat**2)/norm_y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note: From this view, as from the matrix algebra view, it doesn't really matter how many predictors there are in the design (that is, how many dimensions are are projecting into), as long as it's less than the number of dimensions in the data.  Thus, we don't really need to worry about the distinction between *simple* and *multiple* regression from this context.\n",
    "\n",
    "We also haven't said anything about the nature of the design matrix, such as the relations between the regressors.  For now we will simply note that the design matrix must be full rank; that is, no regressor is an exact linear combination of other regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting OLS to data\n",
    "\n",
    "Now let's use the SRO dataset to assess the degree to which household income is predicted by fluid intelligence (as measured using the Raven's progressive matrices) and delay discounting (as measured using the Kirby intertemporal choice task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SRO_data(SRO_datadir='./data/SRO', vars=None):\n",
    "    \"\"\"\n",
    "    load Eisenberg et al. dataset\n",
    "    - by default, load the summary variables from meaningful_variables.csv \n",
    "    and the demographic/health data from demographics_health.csv\n",
    "    \n",
    "    vars: specific variables to return (if None, return all)\n",
    "    \"\"\"\n",
    "\n",
    "    mvars = pd.read_csv(os.path.join(SRO_datadir, 'meaningful_variables.csv'), index_col=0)\n",
    "    dvars = pd.read_csv(os.path.join(SRO_datadir, 'demographic_health.csv'), index_col=0)\n",
    "    \n",
    "    alldata = mvars.join(dvars)\n",
    "    if vars is not None:\n",
    "        assert isinstance(vars, list)\n",
    "        alldata = alldata[vars]\n",
    "    return(alldata)\n",
    "\n",
    "\n",
    "SROdata = get_SRO_data(vars=['kirby.percent_patient', 'ravens.score', 'HouseholdIncome'])\n",
    "# filter out zero income and unreasonably small values\n",
    "SROdata = SROdata.query('HouseholdIncome>500')\n",
    "SROdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(SROdata['HouseholdIncome'], 50)\n",
    "plt.xlabel('Household income')\n",
    "plt.ylabel('count')\n",
    "plt.savefig('sro_income_hist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.hist(SROdata['kirby.percent_patient'], 18)\n",
    "plt.xlabel('Percent patient responding')\n",
    "plt.ylabel('count')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(SROdata['ravens.score'], 18)\n",
    "plt.xlabel('Ravens score')\n",
    "plt.ylabel('count')\n",
    "plt.savefig('sro_kirby_ravens_hist.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's fit a linear regression model to the full dataset, using the statstmodels `OLS()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = SROdata['HouseholdIncome'].copy()\n",
    "X = SROdata[['kirby.percent_patient', 'ravens.score']].copy()\n",
    "X['intercept'] = 1\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that both tasks are significantly associated with the outcome, though they account for a relatively small amount of variance (about 4%).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression diagnostics\n",
    "\n",
    "It's always a good idea to make sure that our model is well specified and that the assumptions of our model are satisfied.  First, let's check whether the model is well specified, by plotting the residuals and squared residuals against the X variables and the fitted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_df = X.copy()\n",
    "del ols_df['intercept']\n",
    "ols_df['yhat'] = results.fittedvalues\n",
    "ols_df['resid'] = results.resid\n",
    "ols_df['resid**2'] = results.resid**2\n",
    "\n",
    "sns.pairplot(ols_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the residuals to the X variables and fitted values there is no clear structure, suggesting that the model is not badly specified.  If clear strucure is present in the residuals (e.g. bimodal distribution) then this suggests that the model is not well specified - that is, that there is some other variable that should be included in the model that will render the residuals normal.\n",
    "\n",
    "The OLS procedure does not require any assumptions regarding the distribution of the errors, but the standard inference procedures for the regression parameters requires the assumption that errors are normally distributed with constant variance.  The Jarque-Bera test reported in the statsmodels output is highly significant, suggesting that there is a lack of normality in the residuals. However, these kinds of tests for normality are often discouraged because they may be overly sensitive.  We can also visualize normality of residuals using a Q-Q plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(results.resid, scipy.stats.norm, fit=True, line='45')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be substantial non-normality in the residuals. One way to address this is by transforming the y values, which are heavily skewed.  There are various transforms that one can use for skewed data, such as the log or square root, but we will use the Box-Cox transformation procedure, which is an adaptive procedure that determine the optimal transform of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = scipy.stats.boxcox(SROdata['HouseholdIncome'].copy())[0]\n",
    "X = SROdata[['kirby.percent_patient', 'ravens.score']].copy()\n",
    "X['intercept'] = 1\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_df = X.copy()\n",
    "del ols_df['intercept']\n",
    "ols_df['yhat'] = results.fittedvalues\n",
    "ols_df['resid'] = results.resid\n",
    "ols_df['resid**2'] = results.resid**2\n",
    "\n",
    "sns.pairplot(ols_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(results.resid, scipy.stats.norm, fit=True, line='45')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals now appear to be much better approximated by a normal distribution.  The main drawback of transformations is that the regression parameters are no longer intepretable in terms of units of the original Y variable.  We will see later how one can also use a linear model (known as a *generalized linear model*) that allows for error distributions other then the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collinearity in the design matrix\n",
    "\n",
    "Collinearity between regressors in the design does not affect the overall fit of the model, but it does affect the variance of the parameter estimates.  This is best seen through simulation.  Let's generate two regressors and an outcome based on the common variance, varying the level of correlation between the regressors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects = 100\n",
    "corr = 0.5\n",
    "noise_sd = 1\n",
    "\n",
    "corrvals = np.arange(0, 1, 0.1)\n",
    "\n",
    "beta = [3, -1, 0]\n",
    "nruns = 100\n",
    "\n",
    "scores = pd.DataFrame({'corr': np.zeros(nruns*len(corrvals)),\n",
    "                       'beta1': np.zeros(nruns*len(corrvals)),\n",
    "                       'beta2': np.zeros(nruns*len(corrvals)),\n",
    "                       'intercept': np.zeros(nruns*len(corrvals)),\n",
    "                       'score': np.zeros(nruns*len(corrvals)),\n",
    "                       'VIF_beta1': np.zeros(nruns*len(corrvals))})\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "ctr = 0\n",
    "for i, corr in enumerate(corrvals):\n",
    "    for j in range(nruns):\n",
    "        X = rng.multivariate_normal(\n",
    "                mean=[0, 0],\n",
    "                cov=[[1, corr], [corr, 1]], \n",
    "                size=n_subjects)\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        # compute variance inflation factor - see below\n",
    "        scores.loc[ctr, 'VIF_beta1'] = variance_inflation_factor(X, 1)\n",
    "        \n",
    "        y = X.dot(beta) + rng.randn(X.shape[0])*noise_sd\n",
    "        lr.fit(X, y)\n",
    "        \n",
    "        scores.loc[ctr, 'score'] = lr.score(X, y)\n",
    "        scores.loc[ctr, 'corr'] = corr\n",
    "        scores.loc[ctr, 'beta1'] = lr.coef_[1]\n",
    "        ctr += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the parameter estimates for beta 1 and see how they relate to the actual value, as a function of the correlation between regressors.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vplot = sns.violinplot(x=\"corr\", y=\"beta1\", data=scores)\n",
    "vplot.plot([-0.5, 9.5], [beta[1], beta[1]], 'k--')\n",
    "locs, labels = plt.xticks()\n",
    "_ = plt.xticks(locs,\n",
    "               rotation=45, \n",
    "               labels=['%.01f' % i for i in np.unique(scores['corr'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the correlation in the design matrix did not cause any bias, but it did increase the variability of the parameter estimates. This follows a specific pattern, known as the *variance inflation factor*:\n",
    "\n",
    "$$ VIF = \\frac{1}{(1 - R^2)} $$\n",
    "\n",
    "Let's plot the observed variance against the variance that we would expect by applying the VIF to the observed variance for the uncorrelated case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize simulation runs by correlation values\n",
    "beta1_summary = scores[['corr', 'beta1', 'VIF_beta1']].groupby('corr').describe()\n",
    "\n",
    "# plot variance of parameter values\n",
    "plt.plot(beta1_summary.index, beta1_summary.loc[:, ('beta1',   'std')]**2)\n",
    "\n",
    "# get prediction from VIF\n",
    "pred_std = (beta1_summary.loc[0, ('beta1',   'std')]**2) * beta1_summary.loc[:, ('VIF_beta1',   'mean')] \n",
    "\n",
    "# plot predicted\n",
    "plt.plot(beta1_summary.index, pred_std, 'k--')\n",
    "plt.legend(['observed variance', 'predicted variance from VIF'])\n",
    "plt.xlabel('correlation between regressors')\n",
    "plt.ylabel('Variance in parameter estimates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the observed variance closely follows that predicted by the VIF.  \n",
    "\n",
    "Importantly, the presence of correlations within the design matrix does *not* change the overall fit of the model, which we can see by plotting the distributions of fit scores ($R^2$) for each level of correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"corr\", y=\"score\",\n",
    "               data=scores)\n",
    "locs, labels = plt.xticks()\n",
    "plt.ylabel('R-squared for full model')\n",
    "_ = plt.xticks(locs,\n",
    "               rotation=45, \n",
    "               labels=['%.01f' % i for i in np.unique(scores['corr'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see in the session on regularization how one can improve performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of orthogonalization\n",
    "\n",
    "In the analysis of neuroimaging data using general linear models (GLMs), it is often common to find that regressors of interest\n",
    "are correlated with one another.  While this inflates the variance of the estimated parameters, the foregoing showed us how OLS ensures that the \n",
    "estimated parameters only reflect the unique variance associated with the particular regressor; any shared variance\n",
    "between regressors, while accounted for in the total model variance, is not reflected in the individual parameter \n",
    "estimates.  In general, this is as it should be; when it is not possible to uniquely attribute variance to any\n",
    "particular regressor, then it should be left out.   Later in the course we will encounter other ways to address this issue, in the context of regularized regression.\n",
    "\n",
    "Unfortunately, there is a tendency within the fMRI literature to overthrow this feature of the GLM by \"orthogonalizing\"\n",
    "variables that are correlated.  This, in effect, assigns the shared variance to one of the correlated variables based \n",
    "on the experimenter's decision.  While statistically valid, this raises serious conceptual concerns about the \n",
    "interpretation of the resulting parameter estimates.\n",
    "\n",
    "The first point to make is that, contrary to claims often seen in fMRI papers, the presence of correlated regressors\n",
    "does not require the use of orthogonalization; in fact, in our opinion there are very few cases in which it is appropriate\n",
    "to use orthogonalization, and its use will most often result in problematic conclusions.  See [Mumford et al., 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126255) for a deeper discussion of this issue.\n",
    "\n",
    "As an example of how the GLM deals with correlated regressors and how this is affected by orthogonalization,\n",
    "we first generate some synthetic data to work with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 100\n",
    "corr = 0.5\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "X = rng.multivariate_normal([0, 0], [[1, corr], [corr, 1]], npts)\n",
    "X = X - np.mean(X, 0)  # center variables\n",
    "\n",
    "params = [1, 2]\n",
    "y_noise = 0.2\n",
    "Y = X.dot(params) + y_noise*rng.randn(npts)\n",
    "\n",
    "orth_df = pd.DataFrame({'Y': Y,\n",
    "                        'X0': X[:, 0],\n",
    "                        'X1': X[:, 1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the relation between each X variable and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(orth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the parameters for the two columns in X using linear regression.  They should come out very close\n",
    "to the values specified for params above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include rcond=None to suppress annoying warning\n",
    "params_est = np.linalg.lstsq(X, Y, rcond=None)[0]\n",
    "\n",
    "print(params_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's orthogonalize the second regressor (X1) with respect to the first (X0) and create a new orthogonalized design matrix X_orth. One way to do this is to fit a regression and then take the residuals; this is known as Gram-Schmidt orthogonalization.  We should see that the orthogonalized design matrix becomes uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_slope = np.linalg.lstsq(X[:, 0].reshape((npts, 1)), X[:, 1].reshape((npts, 1)), rcond=None)[0]\n",
    "\n",
    "X_orth = X.copy()\n",
    "\n",
    "X_orth[:, 1] = X[:, 1] - X[:, 0] * x0_slope\n",
    "\n",
    "print('Correlation matrix for original design matrix')\n",
    "print(np.corrcoef(X.T))\n",
    "\n",
    "print('Correlation matrix for orthogonalized design matrix')\n",
    "print(np.corrcoef(X_orth.T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the parameter estimates for the orthogonalized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_est_orth = np.linalg.lstsq(X_orth, Y, rcond=None)[0]\n",
    "\n",
    "print(params_est_orth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we see is that the parameter estimate for the orthogonalized regressor *did not change*!  Instead, the parameter estimate for the orthogonalized-against regressor changed, because all of the common variance is now being assigned to this regressor. Note that the interpretation of this effect is different than before: The effect for X0_orth is \"the unique effect of X0 along with the common effect of X0 and X1\".  One has to be very careful in using orthogonalization, and in general it is not a good idea ([Mumford et al., 2015](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126255))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation with regression\n",
    "\n",
    "To assess the quality of the regression model, we can use cross-validation to assess out-of-sample fit.  Doing crossvalidation in the context of regression is tricky, because many standard crossvalidation techniques will result in imbalanced distributions of the outcome variable across folds, leading to poor performance.  For this reason, it is suggested to either use a technique that explicitly balances the distribution of the Y variable across folds, or to use a ShuffleSplit method if the dataset is large enough to allow a large proportion of test examples.  Here we will use the `BalancedKFold` function (defined in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# set up SRO data from before:\n",
    "# we don't need intercept column since it's fit by sklearn\n",
    "y = SROdata['HouseholdIncome'].copy().values\n",
    "X = SROdata[['kirby.percent_patient', 'ravens.score']].copy().values\n",
    "\n",
    "kf = BalancedKFold(4)\n",
    "\n",
    "predicted = np.zeros((X.shape[0], 1))\n",
    "lr = LinearRegression()\n",
    "\n",
    "for train_index, test_index in kf.split(X, y, seed=1):\n",
    "    # create separate train and test datasets for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # train the model\n",
    "    lr.fit(X_train, y_train)\n",
    "    predicted[test_index, 0] = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_r2 = r2_score(y, predicted)\n",
    "observed_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sounds like a fairly small number, but predicting even 3% of the variance in an important outcome could be scientifically useful; that is a scientific and practical question rather than a statistical question.\n",
    "\n",
    "In order to assess the degree to which this value is greater than that expected by chance, we can use a randomization approach in which we repeatedly run this analysis, each time randomly shuffling the order of outcome variable. This breaks the relationship between X and y, essentially showing us the expected distribution under the null hypothesis of no relationship.  *NOTE*: This requires the assumption that rows in the dataset are exchangeable, which should be fulfilled when the individuals are random samples from a population. One would not want to use this for, e.g., timeseries data where the there are substantial correlations between subsequent rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a function to run the CV once\n",
    "\n",
    "def run_SRO_cv(X, y, score_fun=r2_score,\n",
    "               seed = None, shuffle=False):\n",
    "    kf = BalancedKFold(4)\n",
    "    if shuffle:\n",
    "        y = y.copy()\n",
    "        np.random.shuffle(y)\n",
    "    predicted = np.zeros((X.shape[0], 1))\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y, seed=seed):\n",
    "        # create separate train and test datasets for this fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # train the model\n",
    "        lr.fit(X_train, y_train)\n",
    "        predicted[test_index, 0] = lr.predict(X_test)\n",
    "\n",
    "    return(r2_score(y, predicted))\n",
    "\n",
    "run_SRO_cv(X, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the null distribution by repeatedly running this function with `shuffle=True`. Since we are looking at the tails, it's generally good to run a large number of randomizations, at least 2500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nruns = 2500\n",
    "r2_rand = np.zeros(nruns)\n",
    "\n",
    "for i in range(nruns):\n",
    "    \n",
    "    r2_rand[i] = run_SRO_cv(X, y, 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the proportion of randomization runs that have as large or larger $R^2$ as the observed data without shuffling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval = 1 - scipy.stats.percentileofscore(r2_rand, observed_r2)/100\n",
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the randomization runs had an $R^2$ as large as or larger than our observed value, meaning that we can reject the null hypothesis of no predictive relationship.  We can see this by plotting the histogram of $R^2$ values from the randomization runs alongside the observed value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.hist(r2_rand, 100)\n",
    "plt.plot([observed_r2, observed_r2], [0, max(h[0])])\n",
    "plt.legend(['observed R2'])\n",
    "plt.xlabel('R2 score')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis shows that the observed degree of predictive accuracy is well above what would be expected by chance alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
